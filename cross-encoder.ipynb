{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHcGlSviANhwZuV/ynBQw8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Apprentissage\n","# Mettre _ pour commencer une fonction indique qu'elle est privée\n","#Je peux pas utiliser pad car"],"metadata":{"id":"QwT9oPu8K-uW","executionInfo":{"status":"ok","timestamp":1737983082962,"user_tz":-60,"elapsed":234,"user":{"displayName":"Salim Abdou Daoura","userId":"06293521395058082300"}}},"execution_count":190,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iYU84azzymzs","executionInfo":{"status":"ok","timestamp":1737984013478,"user_tz":-60,"elapsed":21167,"user":{"displayName":"Salim Abdou Daoura","userId":"06293521395058082300"}},"outputId":"f2c99b5f-70b0-423a-d437-862a571fbcbd"},"execution_count":208,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/Projet Urchadee/Cross encoder :multilabel classification')"],"metadata":{"id":"GWos7RlRy0Tl","executionInfo":{"status":"ok","timestamp":1737984057164,"user_tz":-60,"elapsed":213,"user":{"displayName":"Salim Abdou Daoura","userId":"06293521395058082300"}}},"execution_count":209,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","# Charger le tokenizer de BERT\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # ou 'bert-base-multilingual-cased' pour le multilingue\n","\n","# Définir vos labels et le texte\n","labels = [\"label 1\", \"label 2\", \"label 3\"]  # Liste de vos labels\n","text = \"This is an example of text.\"       # Texte principal\n","\n","# Transformation en une seule chaîne avec \"[CLS]\" avant chaque élément\n","labels_concatenated = \" \".join(f\"{tokenizer.cls_token} {label}\" for label in labels)\n","input = \" \".join([labels_concatenated, f\"{tokenizer.sep_token} {text} {tokenizer.sep_token}\"])\n","encoding = tokenizer(input, return_tensors='pt', add_special_tokens=False)\n","\n","\n","input_ids = encoding['input_ids']\n","attention_mask = encoding['attention_mask']\n","\n","all_cls_positions = (input_ids == tokenizer.cls_token_id).nonzero(as_tuple=True)[1]\n","label_cls_positions = all_cls_positions[:len(labels)]"],"metadata":{"id":"lXSxbI1Qz2rv","executionInfo":{"status":"ok","timestamp":1737967317206,"user_tz":-60,"elapsed":606,"user":{"displayName":"Salim Abdou Daoura","userId":"06293521395058082300"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["label_cls_positions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XlXfDCz3_kw","executionInfo":{"status":"ok","timestamp":1737967317987,"user_tz":-60,"elapsed":9,"user":{"displayName":"Salim Abdou Daoura","userId":"06293521395058082300"}},"outputId":"dc8224e6-d558-4f23-eae6-85a0efbb2f62"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 3, 6])"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","execution_count":202,"metadata":{"id":"goXJzWdrvKSG","executionInfo":{"status":"ok","timestamp":1737983723751,"user_tz":-60,"elapsed":233,"user":{"displayName":"Salim Abdou Daoura","userId":"06293521395058082300"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoModel, AutoTokenizer\n","from torch.nn.utils.rnn import pad_sequence\n","\n","class CrossEncoderModel(nn.Module):\n","    def __init__(self, model_name, max_num_labels):\n","        \"\"\"\n","        Args:\n","            model_name (str): Name of the pretrained model (e.g., \"bert-base-uncased\").\n","            max_num_labels (int): Maximum number of labels that any text can have.\n","        \"\"\"\n","        super(CrossEncoderModel, self).__init__()\n","        # TODO: 1) Load a pretrained transformer model and its tokenizer.\n","        #       2) Initialize a classifier head (nn.Linear) that maps from hidden_size -> 1 (for each label).\n","\n","        # Example:\n","        self.encoder = AutoModel.from_pretrained(model_name)\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.classifier = torch.nn.Linear(self.encoder.config.hidden_size, 1)\n","\n","        self.max_num_labels = max_num_labels\n","\n","    def _build_crossencoder_inputs(self, text, labels):\n","        \"\"\"\n","        Build a single cross-encoder input for one example:\n","            [CLS] label_1 [CLS] label_2 ... [CLS] label_k [SEP] text [SEP]\n","\n","        Returns:\n","            input_ids (torch.LongTensor): Token indices for the concatenated sequence.\n","            attention_mask (torch.LongTensor): Attention mask for that sequence.\n","            label_cls_positions (torch.LongTensor): Indices of the `[CLS]` tokens corresponding to each label.\n","        \"\"\"\n","        # TODO: 1) Construct a string that inserts \"[CLS] \" before each label,\n","        #          then \"[SEP]\" + text + \"[SEP]\" at the end.\n","        #       2) Tokenize this string without automatically adding special tokens,\n","        #          because you are adding them manually.\n","        #       3) Find the positions where `token_id == self.tokenizer.cls_token_id`\n","        #          for the label embeddings.\n","        #       4) Return input_ids, attention_mask, and label_cls_positions.\n","\n","        # Pseudocode:\n","        # label_part = \"\"\n","        # for label in labels:\n","        #     label_part += f\"{self.tokenizer.cls_token} {label} \"\n","        #\n","        # combined_str = f\"{label_part}{self.tokenizer.sep_token} {text} {self.tokenizer.sep_token}\"\n","        #\n","        # encoding = self.tokenizer(\n","        #     combined_str,\n","        #     add_special_tokens=False,\n","        #     return_tensors=\"pt\"\n","        # )\n","        #\n","        # input_ids = ...\n","        # attention_mask = ...\n","        #\n","        # all_cls_positions = (input_ids == self.tokenizer.cls_token_id).nonzero(as_tuple=True)[0]\n","        # label_cls_positions = all_cls_positions[:len(labels)]\n","        #\n","        # return input_ids, attention_mask, label_cls_positions\n","\n","        labels_concatenated = \" \".join(f\"{self.tokenizer.cls_token} {label}\" for label in labels)\n","        combined_str = \" \".join([labels_concatenated, f\"{self.tokenizer.sep_token} {text} {self.tokenizer.sep_token}\"])\n","        encoding = tokenizer(combined_str, add_special_tokens=False, padding=True, return_tensors='pt')\n","\n","        input_ids = encoding['input_ids']\n","        attention_mask = encoding['attention_mask']\n","\n","        all_cls_positions = (input_ids == self.tokenizer.cls_token_id).nonzero(as_tuple=True)[1]\n","        label_cls_positions = all_cls_positions[:len(labels)]\n","\n","        return input_ids, attention_mask, label_cls_positions\n","\n","    def forward(self, texts, batch_labels):\n","        \"\"\"\n","        Args:\n","            texts (List[str]): List of texts with batch size B.\n","            batch_labels (List[List[str]]): List of label-lists for each text.\n","\n","        Returns:\n","            scores (torch.FloatTensor): Shape [B, max_num_labels],\n","                                        containing the predicted relevance for each label.\n","            mask (torch.BoolTensor): Shape [B, max_num_labels],\n","                                     indicating which label positions are valid for each example.\n","        \"\"\"\n","        # TODO: 1) For each example (text + label list), build crossencoder inputs using `_build_crossencoder_inputs`.\n","        #       2) Use the tokenizer's `.pad(...)` to handle variable-length sequences across the batch.\n","        #       3) Pass the padded batch through your transformer encoder.\n","        #       4) Gather the [CLS] embeddings for each label from the `last_hidden_state`.\n","        #       5) Pad them to [B, max_num_labels, hidden_dim].\n","        #       6) Pass through a classifier (linear layer) => [B, max_num_labels].\n","        #       7) Apply sigmoid if doing multi-label classification.\n","        #       8) Return (scores, mask).\n","\n","        # Step-by-step outline:\n","        # 1. Prepare lists for input_ids, attention_masks, label_positions_batch.\n","        # 2. For i in range(len(texts)):\n","        #       input_ids_i, att_mask_i, label_pos_i = self._build_crossencoder_inputs(texts[i], batch_labels[i])\n","        #       store them in lists\n","        #\n","        # 3. Use tokenizer.pad(...) to get padded input_ids and attention_mask for the entire batch.\n","        #\n","        # 4. self.encoder(...) => get last_hidden_state (B, seq_len, hidden_dim).\n","        #\n","        # 5. For each example i, gather the embeddings from the label positions.\n","        # 6. Construct a zero tensor padded_label_embeddings (B, max_num_labels, hidden_dim).\n","        #    Construct a boolean mask (B, max_num_labels) to mark real vs. padded labels.\n","        #\n","        # 7. Run classifier => logits => apply sigmoid => scores.\n","        #\n","        # return (scores, mask)\n","\n","\n","\n","\n","        # # Conversion en listes\n","        # dict_return = {\"input_ids\" : list(liste1), \"attention_mask\" :  list(liste2)}#, \"label_cls_positions\" : list(liste3)}\n","\n","        #return dict_return\n","        B = len(texts)\n","        max_num_labels = self.max_num_labels\n","        hidden_dim = self.encoder.config.hidden_size\n","\n","\n","        input_ids, att_mask, label_pos = [], [], []\n","        for i in range(B):\n","            input_ids_i, att_mask_i, label_pos_i = self._build_crossencoder_inputs(texts[i], batch_labels[i])\n","\n","            input_ids.append(input_ids_i.squeeze(0))\n","            att_mask.append(att_mask_i.squeeze(0))\n","            label_pos.append(label_pos_i.squeeze(0))\n","\n","        inputs = tokenizer.pad({\"input_ids\" : input_ids, \"attention_mask\" : att_mask}, padding=True)\n","        outputs = self.encoder(**inputs)\n","\n","        cls_embeddings = torch.zeros(B, max_num_labels, hidden_dim)\n","        last_hidden_state = outputs.last_hidden_state\n","        mask_labels = torch.zeros(B, max_num_labels, dtype=torch.bool)\n","        for i in range(B):\n","            count = label_pos[i].shape[0]\n","            if count>0:\n","                cls_embeddings[i , :count, :] = last_hidden_state[i, label_pos[i].tolist(), :]\n","                mask_labels[i, :count] = True\n","\n","\n","        scores = self.classifier(cls_embeddings)\n","        scores = torch.sigmoid(scores)\n","\n","        return scores, mask_labels\n","\n","    @torch.no_grad()\n","    def forward_predict(self, texts, labels):\n","        \"\"\"\n","        Args:\n","            texts (List[str]): List of input texts.\n","            labels (List[List[str]]): List of labels corresponding to each text.\n","\n","        Returns:\n","            A list of dictionaries, each containing:\n","               {\n","                   \"text\": str,\n","                   \"scores\": { label: float_score, ... }\n","               }\n","        \"\"\"\n","\n","        scores, masks = self.forward(texts, labels)\n","        results = []\n","        for i, text in enumerate(texts):\n","            text_result = {}\n","            for j, label in enumerate(labels[i]):\n","                if masks[i, j]:\n","                    text_result[label] = float(f\"{scores[i, j].item():.2f}\")\n","            results.append({\"text\": text, \"scores\": text_result})\n","        return results"]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    model_name = \"bert-base-uncased\"\n","    max_num_labels = 5\n","\n","    model = CrossEncoderModel(model_name, max_num_labels)\n","\n","    texts = [\"I love machine learning.\", \"Deep learning models are powerful.\"]\n","    batch_labels = [\n","        [\"AI\", \"Machine Learning\"],\n","        [\"Deep Learning\", \"Neural Networks\", \"AI\"]\n","    ]\n","    a = model.forward_predict(texts, batch_labels)"],"metadata":{"id":"cxEChGhpwJ5t","executionInfo":{"status":"ok","timestamp":1737983814178,"user_tz":-60,"elapsed":1564,"user":{"displayName":"Salim Abdou Daoura","userId":"06293521395058082300"}}},"execution_count":205,"outputs":[]},{"cell_type":"code","source":["a"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eq-dqMCUKuvE","executionInfo":{"status":"ok","timestamp":1737983814181,"user_tz":-60,"elapsed":7,"user":{"displayName":"Salim Abdou Daoura","userId":"06293521395058082300"}},"outputId":"c72e4dc4-880b-4849-82b2-95d7db0dda98"},"execution_count":206,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'text': 'I love machine learning.',\n","  'scores': {'AI': 0.42, 'Machine Learning': 0.43}},\n"," {'text': 'Deep learning models are powerful.',\n","  'scores': {'Deep Learning': 0.49, 'Neural Networks': 0.5, 'AI': 0.51}}]"]},"metadata":{},"execution_count":206}]},{"cell_type":"code","source":[],"metadata":{"id":"K7cPoif9Qdua","executionInfo":{"status":"ok","timestamp":1737983751815,"user_tz":-60,"elapsed":224,"user":{"displayName":"Salim Abdou Daoura","userId":"06293521395058082300"}}},"execution_count":204,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pWijO4RCR59l"},"execution_count":null,"outputs":[]}]}